{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/shubhamitradas/Toxicity-Challenge/blob/master/models/model.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "whcRxSe7nJa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "41e7468c-8669-4453-ff2e-40c2b1d1ba55"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model\n",
        "\"\"\"\n",
        "from typing import Optional, Dict\n",
        "\n",
        "import torch\n",
        "\n",
        "from allennlp.common import Params\n",
        "from allennlp.models import Model\n",
        "from allennlp.data import Vocabulary\n",
        "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder, FeedForward\n",
        "from allennlp.nn.initializers import InitializerApplicator\n",
        "from allennlp.nn.regularizers import RegularizerApplicator\n",
        "import allennlp.nn.util as util\n",
        "from allennlp.training.metrics import BooleanAccuracy\n",
        "\n",
        "from toxic.training.metrics.multilabel_f1 import MultiLabelF1Measure\n",
        "\n",
        "@Model.register(\"toxic\")\n",
        "class ToxicModel(Model):\n",
        "    \"\"\"\n",
        "    toxic model\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab: Vocabulary,\n",
        "                 text_field_embedder: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 classifier_feedforward: FeedForward,\n",
        "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
        "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
        "        super().__init__(vocab, regularizer)\n",
        "\n",
        "        self.text_field_embedder = text_field_embedder\n",
        "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
        "        self.encoder = encoder\n",
        "        self.classifier_feedforward = classifier_feedforward\n",
        "        self.f1 = MultiLabelF1Measure()\n",
        "        self.loss = torch.nn.MultiLabelSoftMarginLoss()\n",
        "\n",
        "        initializer(self)\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'ToxicModel':\n",
        "        embedder_params = params.pop(\"text_field_embedder\")\n",
        "        text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)\n",
        "        encoder = Seq2VecEncoder.from_params(params.pop(\"encoder\"))\n",
        "        classifier_feedforward = FeedForward.from_params(params.pop(\"classifier_feedforward\"))\n",
        "\n",
        "        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n",
        "        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n",
        "\n",
        "        return cls(vocab=vocab,\n",
        "                   text_field_embedder=text_field_embedder,\n",
        "                   encoder=encoder,\n",
        "                   classifier_feedforward=classifier_feedforward,\n",
        "                   initializer=initializer,\n",
        "                   regularizer=regularizer)\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        precision, recall, f1 = self.f1.get_metric(reset)\n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "\n",
        "    def forward(self,\n",
        "                text: Dict[str, torch.Tensor],\n",
        "                labels: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        embedded_text = self.text_field_embedder(text)\n",
        "        mask = util.get_text_field_mask(text)\n",
        "        encoded_text = self.encoder(embedded_text, mask)\n",
        "\n",
        "        logits = self.classifier_feedforward(encoded_text)\n",
        "        probabilities = torch.nn.functional.sigmoid(logits)\n",
        "\n",
        "        output_dict = {\"logits\": logits,\n",
        "                       \"probabilities\": probabilities}\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.loss(logits, labels.squeeze(-1).float())\n",
        "            output_dict[\"loss\"] = loss\n",
        "\n",
        "            predictions = (logits.data > 0.0).long()\n",
        "            label_data = labels.squeeze(-1).data.long()\n",
        "            self.f1(predictions, label_data)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-30a1a80538a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBooleanAccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtoxic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultilabel_f1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLabelF1Measure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"toxic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'toxic'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}