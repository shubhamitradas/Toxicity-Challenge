{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/shubhamitradas/Toxicity-Challenge/blob/master/toxic_pytorch.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "_MilPHBFczWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "c839fba8-2423-48d8-fb5f-3d53cc5fb5fd"
      },
      "cell_type": "code",
      "source": [
        "!pip install http://download.pytorch.org/whl/cpu/torch-0.3.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.1 from http://download.pytorch.org/whl/cpu/torch-0.3.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cpu/torch-0.3.1-cp36-cp36m-linux_x86_64.whl (47.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 47.0MB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.1) (3.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.1) (1.14.5)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-0.3.1\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/4b/8b54ab9d37b93998c81b364557dff9f61972c0f650efa0ceaf470b392740/Pillow-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch->torchvision) (3.12)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.1.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yGBjwUT8dOSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c5a717b8-325a-4242-8276-57e6489bda49"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# Parameters and DataLoaders\n",
        "HIDDEN_SIZE = 100\n",
        "N_LAYERS = 2\n",
        "BATCH_SIZE = 256\n",
        "N_EPOCHS = 3\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 150\n",
        "MAX_NB_WORDS = 10000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "\n",
        "\n",
        "# Some utility functions\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def create_variable(tensor):\n",
        "    # Do cuda() before wrapping with variable\n",
        "    if torch.cuda.is_available():\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "    \n",
        "path = ''\n",
        "EMBEDDING_FILE=path+'glove.840B.300d.txt'\n",
        "TRAIN_DATA_FILE=path+'train_toxic.csv'\n",
        "TEST_DATA_FILE=path+'test_toxic.csv'\n",
        "\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "########################################\n",
        "## process texts in datasets\n",
        "########################################\n",
        "print('Processing text dataset')\n",
        "\n",
        "#Regex to remove all Non-Alpha Numeric and space\n",
        "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
        "\n",
        "#regex to replace all numerics\n",
        "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
        "\n",
        "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    #Remove Special Characters\n",
        "    text=special_character_removal.sub('',text)\n",
        "    \n",
        "    #Replace Numbers\n",
        "    text=replace_numbers.sub('n',text)\n",
        "\n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)\n",
        "\n",
        "\n",
        "\n",
        "class TrainTestDataset(Dataset):\n",
        "\n",
        "\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self, isTrain=True):\n",
        "        train_df = pd.read_csv(TRAIN_DATA_FILE,nrows=900) \n",
        "        test_df = pd.read_csv(TEST_DATA_FILE)\n",
        "        print(\"test size\",test_df.shape)\n",
        "        \n",
        "        list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
        "        list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        labels_train = train_df[list_classes].values\n",
        "        list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n",
        "        \n",
        "        comments = []\n",
        "        for text in list_sentences_train:\n",
        "            comments.append(text_to_wordlist(text))\n",
        "\n",
        "    \n",
        "        test_comments=[]\n",
        "        for text in list_sentences_test:\n",
        "            test_comments.append(text_to_wordlist(text))\n",
        "\n",
        "\n",
        "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "        tokenizer.fit_on_texts(comments + test_comments)\n",
        "\n",
        "        sequences = tokenizer.texts_to_sequences(comments)\n",
        "        test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
        "\n",
        "\n",
        "        word_index = tokenizer.word_index\n",
        "        print('Found %s unique tokens' % len(word_index))\n",
        "\n",
        "\n",
        "        data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "        print('Shape of data tensor:', data.shape)\n",
        "        print('Shape of label tensor:', labels_train.shape)\n",
        "\n",
        "\n",
        "        test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "        print('Shape of test_data tensor:', test_data.shape)\n",
        "        self.train = isTrain\n",
        "        if(self.train == True):\n",
        "            self.len = data.shape[0]\n",
        "        else:\n",
        "            self.len = test_data.shape[0]\n",
        "        self.x_data = torch.LongTensor(data)\n",
        "        self.x_test_data = torch.LongTensor(test_data)\n",
        "        self.y_data = torch.LongTensor(labels_train)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if(self.train == True):\n",
        "            return self.x_data[index], self.y_data[index]\n",
        "        else:\n",
        "            return self.x_test_data[index]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing text dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FUGf9YtPdR_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6cba13ee-711e-4a20-ef95-723ed3a8a8c4"
      },
      "cell_type": "code",
      "source": [
        "train_dataset = TrainTestDataset(isTrain=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=256,\n",
        "                          shuffle=True)\n",
        "\n",
        "\n",
        "test_dataset = TrainTestDataset(isTrain=False)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                          batch_size=1024,\n",
        "                          shuffle=True)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test size (153164, 2)\n",
            "Found 251665 unique tokens\n",
            "Shape of data tensor: (900, 150)\n",
            "Shape of label tensor: (900, 6)\n",
            "Shape of test_data tensor: (153164, 150)\n",
            "test size (153164, 2)\n",
            "Found 251665 unique tokens\n",
            "Shape of data tensor: (900, 150)\n",
            "Shape of label tensor: (900, 6)\n",
            "Shape of test_data tensor: (153164, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PNB1iFKPdVUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    # Our model\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_directions = int(bidirectional) + 1\n",
        "        #print(\"Input to embedding  is \",input_size,hidden_size)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.bn = nn.BatchNorm1d(256)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        # Note: we run this all at once (over the whole input sequence)\n",
        "        # input shape: B x S (input size)\n",
        "        # transpose to make S(sequence) x B (batch)\n",
        "        #print(\"Inside forward1 \",input.shape)\n",
        "        input = input.t()\n",
        "        batch_size = input.size(1)\n",
        "        \n",
        "        #print(batch_size)\n",
        "        \n",
        "\n",
        "        # Make a hidden\n",
        "        hidden = self._init_hidden(batch_size)\n",
        "\n",
        "        # Embedding S x B -> S x B x I (embedding size)\n",
        "        #print(\"Inside forward\")\n",
        "        #print(input.shape)\n",
        "\n",
        "        embedded = self.embedding(input)\n",
        "        #print(embedded.shape)\n",
        "        \n",
        "\n",
        "        # Pack them up nicely\n",
        "        '''gru_input = pack_padded_sequence(\n",
        "            embedded, seq_lengths.data.cpu().numpy())'''\n",
        "        gru_input = embedded\n",
        "        \n",
        "\n",
        "        # To compact weights again call flatten_parameters().\n",
        "        self.gru.flatten_parameters()\n",
        "        output, hidden = self.gru(gru_input, hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        #hidden = self.bn(hidden)\n",
        "\n",
        "        # Use the last layer output as FC's input\n",
        "        # No need to unpack, since we are going to use hidden\n",
        "        fc_output = self.fc(hidden[-1])\n",
        "        #print(\"fc_output\",fc_output)\n",
        "        return fc_output\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
        "                             batch_size, self.hidden_size)\n",
        "        return create_variable(hidden)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "    \n",
        "        total_loss = 0\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "           # get the inputs\n",
        "           inputs, labels = data\n",
        "           # wrap them in Variable\n",
        "           inputs, labels = create_variable(inputs), create_variable(labels)\n",
        "           output = classifier(inputs)\n",
        "           pred = F.softmax(output)\n",
        "           loss = criterion(pred, labels)\n",
        "           #print(\"loss: \",loss.data)\n",
        "           total_loss += loss.data[0]\n",
        "           classifier.zero_grad()\n",
        "           loss.backward()\n",
        "           optimizer.step()\n",
        "        \n",
        "        print(\"total_loss\",total_loss)    \n",
        "        return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Testing cycle\n",
        "def test(name=None):\n",
        "        \n",
        "        output = classifier(data_val)\n",
        "        print(\"In test :\",output.shape)\n",
        "        pred = output #.data.max(1, keepdim=True)[1]\n",
        "        print(pred)\n",
        "        print(labels_val)\n",
        "        '''correct = pred.eq(labels_val.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, train_data_size, 100. * correct / train_data_size))'''\n",
        "        from sklearn import metrics\n",
        "        #roc = metrics.roc_auc_score(labels_val.data.cpu().numpy(), pred.data.cpu().numpy())\n",
        "        #print(\"ROC:\", roc)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlmtQN9HdtRD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b219686a-a8aa-4756-addd-e58f26ff5583"
      },
      "cell_type": "code",
      "source": [
        "classifier = RNNClassifier(MAX_NB_WORDS , HIDDEN_SIZE, 6, N_LAYERS)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n",
        "    classifier = nn.DataParallel(classifier)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    classifier.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "criterion = nn.MultiLabelMarginLoss()\n",
        "\n",
        "start = time.time()\n",
        "print(\"Training for %d epochs...\" % N_EPOCHS)\n",
        "for epoch in range(1, 10 + 1):\n",
        "    # Train cycle\n",
        "    train()\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 3 epochs...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total_loss 15.110389947891235\n",
            "total_loss 4.805549144744873\n",
            "total_loss 2.5375065207481384\n",
            "total_loss 1.8945719599723816\n",
            "total_loss 1.2717264294624329\n",
            "total_loss 0.8551862984895706\n",
            "total_loss 0.7458672672510147\n",
            "total_loss 0.6911358386278152\n",
            "total_loss 0.6703790575265884\n",
            "total_loss 0.6903959214687347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OUPGo7e1fLAJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fdb7c479-3eb0-4ae1-d269-65d7943659f1"
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "predictions = []\n",
        "for i, test_data in enumerate(test_loader, 0):\n",
        "    #print(test_data.shape)\n",
        "    test_data = create_variable(test_data)\n",
        "    output = classifier(test_data)\n",
        "    pred = F.sigmoid(output)\n",
        "    predictions.append(pred.data.cpu().numpy())\n",
        "\n",
        "    \n",
        "predictions = np.vstack(predictions)\n",
        "print(\"===> Raw predictions done. Here is a snippet\")\n",
        "print(predictions)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===> Raw predictions done. Here is a snippet\n",
            "[[0.9699192  0.8078989  0.08396397 0.41501966 0.16176644 0.10449158]\n",
            " [0.98990685 0.80779487 0.08335509 0.25004372 0.15071873 0.03490622]\n",
            " [0.9734914  0.6271875  0.12660392 0.18886292 0.26125896 0.25229114]\n",
            " ...\n",
            " [0.9645737  0.5643977  0.08189034 0.24476935 0.19861893 0.19117911]\n",
            " [0.95705944 0.65933484 0.11777349 0.14895216 0.1911027  0.15711537]\n",
            " [0.9848856  0.7690941  0.06945001 0.2642095  0.13213487 0.36221263]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7bvh5h2PfOXn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a4c221b-4ccb-46eb-da78-378f842330b0"
      },
      "cell_type": "code",
      "source": [
        "print(predictions.shape)\n",
        "sample_submission = pd.read_csv(\"sample_toxic.csv\")\n",
        "print(sample_submission.shape)\n",
        "sample_submission[list_classes] = predictions\n",
        "\n",
        "sample_submission.to_csv('submission_pytorch.csv', index=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(153164, 6)\n",
            "(153164, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qH99jdh6jald",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gzip submission_pytorch.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fxC-ZhCNjhwa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "files.download('submission_pytorch.csv.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}