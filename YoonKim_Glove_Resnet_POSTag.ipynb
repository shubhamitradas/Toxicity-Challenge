{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/shubhamitradas/Toxicity-Challenge/blob/master/YoonKim_Glove_Resnet_POSTag.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DvQ69XQyiXyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0a81ce4-ec23-46ed-b202-c42a747b0273"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM,Bidirectional, Embedding, Dropout, Activation,GlobalMaxPool1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import sys\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "\n",
        "#This code below for the Attention implementation borrowed from various sources.\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        #self.init = initializations.get('glorot_uniform')\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
        "\n",
        "        # features_dim = self.W.shape[0]\n",
        "        # step_dim = x._keras_shape[1]\n",
        "\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "    #print weigthted_input.shape\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        #return input_shape[0], input_shape[-1]\n",
        "        return input_shape[0],  self.features_dim\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "WDH3iFIDiuni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c7aa29ed-d318-4a5b-fb99-89e8c438df1e"
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE_RES  = 'numberbatch-en-17.06.txt' \n",
        "EMBEDDING_FILE  = 'glove.840B.300d.txt'\n",
        "\n",
        "TRAIN_DATA_FILE = 'train_toxic.csv'\n",
        "TEST_DATA_FILE  = 'test_toxic.csv'\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NB_WORDS = 100000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "num_lstm = 100\n",
        "num_dense = 210\n",
        "rate_drop_lstm = 0.25\n",
        "rate_drop_dense = 0.25\n",
        "act = 'relu'\n",
        "\n",
        "########################################\n",
        "## index word vectors,Globe and Resnet\n",
        "########################################\n",
        "print('Indexing word vectors')\n",
        "\n",
        "#Glove Vectors\n",
        "embeddings_index = {}\n",
        "f = open(EMBEDDING_FILE)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = ' '.join(values[:-300])\n",
        "    coefs = np.asarray(values[-300:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "#Resnet Vectors\n",
        "embeddings_index_res = {}\n",
        "f = open(EMBEDDING_FILE_RES)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = ' '.join(values[:-300])\n",
        "    coefs = np.asarray(values[-300:], dtype='float32')\n",
        "    embeddings_index_res[word] = coefs\n",
        "f.close()\n",
        "\n",
        "\n",
        "print('Total %s word vectors for Glove.' % len(embeddings_index))\n",
        "print('Total %s word vectors for Resnet.' % len(embeddings_index_res))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors\n",
            "Total 2195896 word vectors for Glove.\n",
            "Total 417195 word vectors for Resnet.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KA_yh19yPV_U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we include both the pre-trained word vectors from Glove and Resnet and concatenate them.\n",
        "The last column of the embedding matrix  also includes the POS Tag information from NLTK,just trying to make the embedding layer more feature rich."
      ]
    },
    {
      "metadata": {
        "id": "id7odf-j0BR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c6fb34ba-65b9-44dd-bc11-913fa872fa39"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "\n",
        "########################################\n",
        "## process texts in datasets\n",
        "########################################\n",
        "print('Processing text dataset')\n",
        "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
        "test_df = pd.read_csv(TEST_DATA_FILE)\n",
        "\n",
        "\n",
        "#Regex to remove all Non-Alpha Numeric and space\n",
        "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
        "\n",
        "#regex to replace all numerics\n",
        "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
        "\n",
        "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    #print(\"Raw text \",text)\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    #Remove Special Characters\n",
        "    text=special_character_removal.sub('',text)\n",
        "    \n",
        "    #Replace Numbers\n",
        "    text=replace_numbers.sub('n',text)\n",
        "\n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    #print(\"Processed text \",text)\n",
        "    return(text)\n",
        "\n",
        "\n",
        "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train_df[list_classes].values\n",
        "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n",
        "\n",
        "\n",
        "comments = []\n",
        "for text in list_sentences_train:\n",
        "    comments.append(text_to_wordlist(text))\n",
        "    \n",
        "print(\"Comments :\",comments)    \n",
        "\n",
        "    \n",
        "test_comments=[]\n",
        "for text in list_sentences_test:\n",
        "    test_comments.append(text_to_wordlist(text))\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(comments + test_comments)\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(comments)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
        "\n",
        "print(sequences)\n",
        "#print(test_comments)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', y.shape)\n",
        "\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of test_data tensor:', test_data.shape)\n",
        "\n",
        "\n",
        "\n",
        "pos_code_map={'CC':1,'CD':2,'DT':3,'EX':4,'FW':5,'IN':6,'JJ':7,'JJR':8,'JJS':9,'LS':10,'MD':11,'NN':12,'NNS':13,\n",
        "'NNP':14,'NNPS':15,'PDT':16,'POS':17,'PRP':18,'PRP$':19,'RB':20,'RBR':21,'RBS':22,'RP':23,'SYM':24,'TO':25,'UH':26,\n",
        "'VB':27,'VBD':28,'VBG':29,'VBN':30,'VBP':31,'VBZ':32,'WDT':33,'WP':34,'WP$':35,'WRB':39,'?':40,'UNK':41}\n",
        "\n",
        "code_pos_map = {v: k for k, v in  pos_code_map.items()}\n",
        "\n",
        "def convert(tag):\n",
        "    try:\n",
        "        code=pos_code_map[tag]\n",
        "    except:\n",
        "        code='UNK'\n",
        "            \n",
        "    return code\n",
        "\n",
        "########################################\n",
        "## prepare embeddings\n",
        "########################################\n",
        "print('Preparing embedding matrix')\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM+EMBEDDING_DIM+1))\n",
        "for word, i in word_index.items():\n",
        "    if i >= MAX_NB_WORDS:\n",
        "         continue     \n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    embedding_vector_res = embeddings_index_res.get(word)\n",
        "    tag = nltk.tag.pos_tag([word])\n",
        "    tag_no = convert(tag[0][1])\n",
        "    if embedding_vector  is not None:\n",
        "        if embedding_vector_res is not None :\n",
        "            embedding_vector = np.append(embedding_vector,embedding_vector_res)\n",
        "            embedding_vector = np.append(embedding_vector,int(tag_no))\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "   \n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing text dataset\n",
            "Comments : "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (159571, 100)\n",
            "Shape of label tensor: (159571, 6)\n",
            "Shape of test_data tensor: (153164, 100)\n",
            "Preparing embedding matrix\n",
            "Null word embeddings: 41407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "omGfbiS3viGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bfcb4c26-51b8-45ab-94e8-72cb74071a96"
      },
      "cell_type": "code",
      "source": [
        "train_df[\"comment_text\"].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Explanation\\nWhy the edits made under my usern...\n",
              "1    D'aww! He matches this background colour I'm s...\n",
              "2    Hey man, I'm really not trying to edit war. It...\n",
              "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
              "4    You, sir, are my hero. Any chance you remember...\n",
              "Name: comment_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "giZ1b-sGBisn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6b9ee101-780e-4d22-8d07-83eb77cca28e"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input, CuDNNLSTM,CuDNNGRU,Bidirectional, Embedding,ActivityRegularization\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "########################################\n",
        "## sample train/validation data\n",
        "########################################\n",
        "np.random.seed(1234)\n",
        "perm = np.random.permutation(len(data))\n",
        "idx_train = perm[:int(len(data)*(1-VALIDATION_SPLIT))]\n",
        "idx_val = perm[int(len(data)*(1-VALIDATION_SPLIT)):]\n",
        "\n",
        "data_train=data[idx_train]\n",
        "labels_train=y[idx_train]\n",
        "print(data_train.shape,labels_train.shape)\n",
        "\n",
        "data_val=data[idx_val]\n",
        "labels_val=y[idx_val]\n",
        "\n",
        "print(data_val.shape,labels_val.shape)\n",
        "\n",
        "  \n",
        "def swish(x):\n",
        "    return (K.sigmoid(x) * x)\n",
        "  \n",
        "\n",
        "def selu(x):\n",
        "    alpha = 1.6732632423543772848170429916717\n",
        "    scale = 1.0507009873554804934193349852946\n",
        "    return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n",
        "  \n",
        "act = 'relu'  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch, score))\n",
        "            \n",
        "            \n",
        "ra_val = RocAucEvaluation(validation_data=(data_val, labels_val), interval=1)\n",
        "\n",
        "\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if  epoch > 7:\n",
        "        lr = K.get_value(model.optimizer.lr)\n",
        "        K.set_value(model.optimizer.lr, lr*.9)\n",
        "        print(\"lr changed to {}\".format(lr*.9))\n",
        "    return K.get_value(model.optimizer.lr)\n",
        "\n",
        "lr_decay = LearningRateScheduler(scheduler)\n",
        "\n",
        "lrate = 0.0005\n",
        "epochs = 10\n",
        "num_dense = 210\n",
        "num_lstm = 70\n",
        "decay = lrate/epochs\n",
        "#sgd = SGD(lr=lrate, momentum=0.90, decay=decay, nesterov=False)\n",
        "#rmsprop = RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(143613, 100) (143613, 6)\n",
            "(15958, 100) (15958, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ausXMAw0llE-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1581
        },
        "outputId": "ad3e1a21-d616-45ef-a83a-85236c953c99"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "###################### Model  ###################################\n",
        "\n",
        "\n",
        "from keras.optimizers import RMSprop,SGD,Adam,Nadam\n",
        "from keras.layers import Dense, Input, LSTM,Bidirectional, Embedding, Dropout, Activation,GlobalMaxPooling1D,SpatialDropout1D,GlobalAveragePooling1D,Concatenate\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "## define the model structure\n",
        "########################################\n",
        "embedding_layer = Embedding(nb_words,\n",
        "        EMBEDDING_DIM+EMBEDDING_DIM+1,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_SEQUENCE_LENGTH,\n",
        "        trainable=False)\n",
        "\n",
        "########yoon kim###############################\n",
        "from keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\n",
        "from keras.layers import Dense, GlobalMaxPooling1D\n",
        "conv_filters = 128\n",
        "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
        "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "emb= embedding_layer(comment_input)\n",
        "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3)(emb)\n",
        "btch1_1 = BatchNormalization()(conv1_1)\n",
        "drp1_1  = Dropout(0.2)(btch1_1)\n",
        "actv1_1 = Activation(act)(drp1_1)\n",
        "glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
        "\n",
        "conv1_2 = Conv1D(filters=conv_filters, kernel_size=4)(emb)\n",
        "btch1_2 = BatchNormalization()(conv1_2)\n",
        "drp1_2  = Dropout(0.2)(btch1_2)\n",
        "actv1_2 = Activation(act)(drp1_2)\n",
        "glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
        "\n",
        "conv1_3 = Conv1D(filters=conv_filters, kernel_size=5)(emb)\n",
        "btch1_3 = BatchNormalization()(conv1_3)\n",
        "drp1_3  = Dropout(0.2)(btch1_3)\n",
        "actv1_3 = Activation(act)(drp1_3)\n",
        "glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
        "\n",
        "conv1_4 = Conv1D(filters=conv_filters, kernel_size=6)(emb)\n",
        "btch1_4 = BatchNormalization()(conv1_4)\n",
        "drp1_4  = Dropout(0.2)(btch1_4)\n",
        "actv1_4 = Activation(act)(drp1_4)\n",
        "glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
        "\n",
        "# Gather all convolution layers\n",
        "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
        "drp1 = Dropout(0.2)(cnct)\n",
        "\n",
        "dns1  = Dense(32, activation=act)(drp1)\n",
        "btch1 = BatchNormalization()(dns1)\n",
        "drp2  = Dropout(0.2)(btch1)\n",
        "\n",
        "output_layer = Dense(6, activation=\"sigmoid\")(drp2)\n",
        "model = Model(inputs=comment_input, outputs=output_layer)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=Adam(),#(clipvalue=1, clipnorm=1),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "#########################################LearningRateScheduler#######\n",
        "\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "early_stopping =EarlyStopping(monitor='acc', patience=3)\n",
        "bst_model_path = \"relu\" + '.h5'\n",
        "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "#model.load_weights(bst_model_path)\n",
        "hist = model.fit(data_train, labels_train, \\\n",
        "        validation_data=(data_val, labels_val), \\\n",
        "        epochs=epochs, batch_size=256,verbose=2, shuffle=True, \\\n",
        "         callbacks=[model_checkpoint,lr_decay])\n",
        "         \n",
        "model.load_weights(bst_model_path)\n",
        "bst_val_score = min(hist.history['val_loss'])\n",
        "\n",
        "#######################################\n",
        "## make the submission\n",
        "########################################\n",
        "print('Start making the submission before fine-tuning')\n",
        "\n",
        "from sklearn import metrics\n",
        "valid_preds = model.predict(data_val, verbose=0)\n",
        "roc = metrics.roc_auc_score(labels_val, valid_preds)\n",
        "print(\"ROC:\", roc)\n",
        "\n",
        "STAMP = 'glove_resnet_postag_concat_%.4f'%(roc)\n",
        "\n",
        "y_test = model.predict([test_data], batch_size=1024, verbose=2)\n",
        "\n",
        "sample_submission = pd.read_csv(\"sample_toxic.csv\")\n",
        "sample_submission[list_classes] = y_test\n",
        "\n",
        "sample_submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 100, 601)     60100000    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 98, 128)      230912      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 97, 128)      307840      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 96, 128)      384768      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 95, 128)      461696      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 98, 128)      512         conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 97, 128)      512         conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 96, 128)      512         conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 95, 128)      512         conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 98, 128)      0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 97, 128)      0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 96, 128)      0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 95, 128)      0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 98, 128)      0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 97, 128)      0           dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 96, 128)      0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 95, 128)      0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_5 (GlobalM (None, 128)          0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_6 (GlobalM (None, 128)          0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_7 (GlobalM (None, 128)          0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_8 (GlobalM (None, 128)          0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 512)          0           global_max_pooling1d_5[0][0]     \n",
            "                                                                 global_max_pooling1d_6[0][0]     \n",
            "                                                                 global_max_pooling1d_7[0][0]     \n",
            "                                                                 global_max_pooling1d_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 512)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 32)           16416       dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32)           128         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32)           0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 6)            198         dropout_12[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 61,504,006\n",
            "Trainable params: 1,402,918\n",
            "Non-trainable params: 60,101,088\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/10\n",
            " - 79s - loss: 0.2357 - acc: 0.9205 - val_loss: 0.0624 - val_acc: 0.9807\n",
            "Epoch 2/10\n",
            " - 77s - loss: 0.0595 - acc: 0.9799 - val_loss: 0.0515 - val_acc: 0.9812\n",
            "Epoch 3/10\n",
            " - 77s - loss: 0.0525 - acc: 0.9811 - val_loss: 0.0509 - val_acc: 0.9806\n",
            "Epoch 4/10\n",
            " - 77s - loss: 0.0494 - acc: 0.9818 - val_loss: 0.0493 - val_acc: 0.9814\n",
            "Epoch 5/10\n",
            " - 77s - loss: 0.0470 - acc: 0.9824 - val_loss: 0.0474 - val_acc: 0.9819\n",
            "Epoch 6/10\n",
            " - 78s - loss: 0.0449 - acc: 0.9832 - val_loss: 0.0467 - val_acc: 0.9822\n",
            "Epoch 7/10\n",
            " - 77s - loss: 0.0429 - acc: 0.9837 - val_loss: 0.0458 - val_acc: 0.9828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/10\n",
            " - 77s - loss: 0.0411 - acc: 0.9842 - val_loss: 0.0487 - val_acc: 0.9818\n",
            "Epoch 9/10\n",
            "lr changed to 0.0009000000427477062\n",
            " - 77s - loss: 0.0387 - acc: 0.9851 - val_loss: 0.0455 - val_acc: 0.9829\n",
            "Epoch 10/10\n",
            "lr changed to 0.0008100000384729356\n",
            " - 77s - loss: 0.0370 - acc: 0.9858 - val_loss: 0.0479 - val_acc: 0.9825\n",
            "Start making the submission before fine-tuning\n",
            "ROC: 0.9861232269195801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oMP9MlSnitMf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilK_UnigBUxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "df73e57e-a206-4c33-bd75-e59362c66341"
      },
      "cell_type": "code",
      "source": [
        "#########################################LearningRateScheduler#######\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import keras.backend as K\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if  epoch > 7:\n",
        "        lr = K.get_value(model_attn.optimizer.lr)\n",
        "        K.set_value(model_attn.optimizer.lr, lr*.9)\n",
        "        print(\"lr changed to {}\".format(lr*.9))\n",
        "    return K.get_value(model_attn.optimizer.lr)\n",
        "\n",
        "lr_decay = LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "\n",
        "###################### Attention Model  ###################################\n",
        "\n",
        "from keras.optimizers import RMSprop,SGD,Adam,Nadam\n",
        "from keras.layers import Bidirectional, Dropout, CuDNNGRU\n",
        "recurrent_units = 84\n",
        "\n",
        "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences= embedding_layer(comment_input)\n",
        "x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(embedded_sequences)\n",
        "x = Attention(MAX_SEQUENCE_LENGTH)(embedded_sequences)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation=act)(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(32, activation=act)(x)\n",
        "output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
        "model_attn = Model(inputs=comment_input, outputs=output_layer)\n",
        "\n",
        "\n",
        "model_attn.compile(loss='binary_crossentropy',\n",
        "                  optimizer=Adam(lr=0.0005),#(clipvalue=1, clipnorm=1),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print(model_attn.summary())\n",
        "\n",
        "\n",
        "early_stopping =EarlyStopping(monitor='acc', patience=3)\n",
        "bst_model_path = \"relu\" + '.h5'\n",
        "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "#model.load_weights(bst_model_path)\n",
        "hist = model_attn.fit(data_train, labels_train, \\\n",
        "        validation_data=(data_val, labels_val), \\\n",
        "        epochs=epochs, batch_size=256,verbose=2, shuffle=True, \\\n",
        "         callbacks=[model_checkpoint,lr_decay])\n",
        "         \n",
        "model_attn.load_weights(bst_model_path)\n",
        "bst_val_score = min(hist.history['val_loss'])\n",
        "\n",
        "#######################################\n",
        "## make the submission\n",
        "########################################\n",
        "print('Start making the submission before fine-tuning')\n",
        "\n",
        "from sklearn import metrics\n",
        "valid_preds = model_attn.predict(data_val, verbose=0)\n",
        "roc = metrics.roc_auc_score(labels_val, valid_preds)\n",
        "print(\"ROC:\", roc)\n",
        "\n",
        "STAMP = 'glove_resnet_postag_attn_%.4f'%(roc)\n",
        "\n",
        "y_test = model_attn.predict([test_data], batch_size=1024, verbose=2)\n",
        "\n",
        "sample_submission = pd.read_csv(\"sample_toxic.csv\")\n",
        "sample_submission[list_classes] = y_test\n",
        "\n",
        "sample_submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 100, 601)          60100000  \n",
            "_________________________________________________________________\n",
            "attention_7 (Attention)      (None, 601)               701       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 601)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 64)                38528     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 60,141,507\n",
            "Trainable params: 41,507\n",
            "Non-trainable params: 60,100,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/10\n",
            " - 9s - loss: 0.1672 - acc: 0.9591 - val_loss: 0.0720 - val_acc: 0.9755\n",
            "Epoch 2/10\n",
            " - 8s - loss: 0.0668 - acc: 0.9776 - val_loss: 0.0604 - val_acc: 0.9787\n",
            "Epoch 3/10\n",
            " - 7s - loss: 0.0592 - acc: 0.9794 - val_loss: 0.0564 - val_acc: 0.9797\n",
            "Epoch 4/10\n",
            " - 8s - loss: 0.0561 - acc: 0.9803 - val_loss: 0.0542 - val_acc: 0.9803\n",
            "Epoch 5/10\n",
            " - 8s - loss: 0.0541 - acc: 0.9807 - val_loss: 0.0530 - val_acc: 0.9805\n",
            "Epoch 6/10\n",
            " - 7s - loss: 0.0523 - acc: 0.9812 - val_loss: 0.0521 - val_acc: 0.9809\n",
            "Epoch 7/10\n",
            " - 8s - loss: 0.0513 - acc: 0.9814 - val_loss: 0.0510 - val_acc: 0.9810\n",
            "Epoch 8/10\n",
            " - 7s - loss: 0.0505 - acc: 0.9817 - val_loss: 0.0504 - val_acc: 0.9812\n",
            "Epoch 9/10\n",
            "lr changed to 0.0004500000213738531\n",
            " - 8s - loss: 0.0498 - acc: 0.9818 - val_loss: 0.0498 - val_acc: 0.9811\n",
            "Epoch 10/10\n",
            "lr changed to 0.0004050000192364678\n",
            " - 8s - loss: 0.0491 - acc: 0.9821 - val_loss: 0.0495 - val_acc: 0.9812\n",
            "Start making the submission before fine-tuning\n",
            "ROC: 0.9789929684113817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ptIUL3iDj5uv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a16f8c72-2fd8-41e7-914e-8a3f08ebb793"
      },
      "cell_type": "code",
      "source": [
        "!ls -lrt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 6829700\r\n",
            "drwxr-xr-x 1 root root       4096 Jun  4 04:03 datalab\r\n",
            "-rw-r--r-- 1 root root    6279782 Jun  4 04:03 sample_toxic.csv\r\n",
            "-rw-r--r-- 1 root root   68802655 Jun  4 04:03 train_toxic.csv\r\n",
            "-rw-r--r-- 1 root root   60354593 Jun  4 04:04 test_toxic.csv\r\n",
            "-rw-r--r-- 1 root root 5646239124 Jun  4 04:05 glove.840B.300d.txt\r\n",
            "-rw-r--r-- 1 root root  943625690 Jun  4 04:05 numberbatch-en-17.06.txt\r\n",
            "drwxr-xr-x 4 root root       4096 Jun  4 04:06 nltk_data\r\n",
            "-rw-r--r-- 1 root root  246073896 Jun  4 05:25 relu.h5\r\n",
            "-rw-r--r-- 1 root root   22201332 Jun  4 05:27 0.0455_glove_resnet_postag_concat_0.9861.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e2St4f78jy2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ba538fd0-3a2c-46ea-d1ce-f45e2b90aa05"
      },
      "cell_type": "code",
      "source": [
        "!gzip 0.0455_glove_resnet_postag_concat_0.9861.csv"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9t5Q_4EMjJYm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ba1ac811-38a4-400d-a827-7323bad28f8c"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "files.download('0.0455_glove_resnet_postag_concat_0.9861.csv.gz')"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}